import logging
import sys
import os
import requests
import re
import csv
import subprocess
import threading
import time
import json
import argparse
import platform
import shutil
import tarfile
from typing import List, Tuple, Dict
from collections import defaultdict
from charset_normalizer import detect
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pathlib import Path
from packaging import version
import tempfile
import atexit
import stat
import venv
import ast
from bs4 import BeautifulSoup  # æ–°å¢ä¾èµ–ä»¥æ”¯æŒBè„šæœ¬çš„ç½‘é¡µæŠ“å–
from concurrent.futures import ThreadPoolExecutor  # æ–°å¢å¯¼å…¥

# ç¡®ä¿æ—¥å¿—æ–‡ä»¶è·¯å¾„å¯å†™
LOG_FILE = "speedtest.log"
LOG_DIR = os.path.dirname(os.path.abspath(__file__))
try:
    os.makedirs(LOG_DIR, exist_ok=True)
    LOG_PATH = os.path.join(LOG_DIR, LOG_FILE)
    with open(LOG_PATH, 'a', encoding='utf-8') as f:
        pass
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        handlers=[
            logging.FileHandler(LOG_PATH, encoding="utf-8", mode="w"),
            logging.StreamHandler(sys.stdout)  # æ¢å¤ StreamHandler
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—åˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {LOG_PATH}")
except Exception as e:
    print(f"æ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶ {LOG_PATH}: {e}")
    sys.exit(1)

# ç¦ç”¨ stdout ç¼“å†²ï¼Œç¡®ä¿å®æ—¶è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)

# é…ç½®
IP_LIST_FILE = "ip.txt"
IPS_FILE = "ips.txt"
FINAL_CSV = "ip.csv"
INPUT_FILE = "input.csv"
TEMP_FILE = os.path.join(tempfile.gettempdir(), "temp_proxy.csv")
TEMP_FILE_CACHE_DURATION = 3600
INPUT_URLS = [
    "https://bihai.cf/CFIP/CUCC/standard.csv",
    # æ·»åŠ æ›´å¤š URLï¼Œä¾‹å¦‚ï¼š
    # "https://example.com/other_ip_list.csv",
]
WEB_URLS = [
    'https://ip.164746.xyz/ipTop10.html',
    'https://cf.090227.xyz',
]
WEB_PORTS = [443, 2053, 2083, 2087, 2096, 8443]
COUNTRY_CACHE_FILE = "country_cache.json"
GEOIP_DB_PATH = Path("GeoLite2-Country.mmdb")
GEOIP_DB_URL_BACKUP = "https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-Country&license_key={}&suffix=tar.gz"
MAXMIND_LICENSE_KEY = os.getenv("MAXMIND_LICENSE_KEY", "")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/"
}
DESIRED_COUNTRIES = ['TW', 'JP', 'HK', 'SG', 'KR', 'IN', 'KP', 'VN', 'TH', 'MM', 'US']
REQUIRED_PACKAGES = ['requests', 'charset-normalizer', 'geoip2==4.8.0', 'maxminddb>=2.0.0', 'packaging>=21.3', 'bs4']  # æ–°å¢bs4
CONFIG_FILE = ".gitconfig.json"
SSH_KEY_PATH = os.path.expanduser("~/.ssh/id_ed25519")
VENV_DIR = ".venv"

# å›½å®¶ä»£ç å’Œæ ‡ç­¾ï¼ˆä¿æŒä¸Aè„šæœ¬ä¸€è‡´ï¼‰
COUNTRY_LABELS = {
    'JP': ('ğŸ‡¯ğŸ‡µ', 'æ—¥æœ¬'), 'KR': ('ğŸ‡°ğŸ‡·', 'éŸ©å›½'), 'SG': ('ğŸ‡¸ğŸ‡¬', 'æ–°åŠ å¡'),
    'TW': ('ğŸ‡¹ğŸ‡¼', 'å°æ¹¾'), 'HK': ('ğŸ‡­ğŸ‡°', 'é¦™æ¸¯'), 'MY': ('ğŸ‡²ğŸ‡¾', 'é©¬æ¥è¥¿äºš'),
    'TH': ('ğŸ‡¹ğŸ‡­', 'æ³°å›½'), 'ID': ('ğŸ‡®ğŸ‡©', 'å°åº¦å°¼è¥¿äºš'), 'PH': ('ğŸ‡µğŸ‡­', 'è²å¾‹å®¾'),
    'VN': ('ğŸ‡»ğŸ‡³', 'è¶Šå—'), 'IN': ('ğŸ‡®ğŸ‡³', 'å°åº¦'), 'MO': ('ğŸ‡²ğŸ‡´', 'æ¾³é—¨'),
    'KH': ('ğŸ‡°ğŸ‡­', 'æŸ¬åŸ”å¯¨'), 'LA': ('ğŸ‡±ğŸ‡¦', 'è€æŒ'), 'MM': ('ğŸ‡²ğŸ‡²', 'ç¼…ç”¸'),
    'MN': ('ğŸ‡²ğŸ‡³', 'è’™å¤'), 'KP': ('ğŸ‡µğŸ‡µ', 'æœé²œ'), 'US': ('ğŸ‡ºğŸ‡¸', 'ç¾å›½'),
    'GB': ('ğŸ‡¬ğŸ‡§', 'è‹±å›½'), 'DE': ('ğŸ‡©ğŸ‡ª', 'å¾·å›½'), 'FR': ('ğŸ‡«ğŸ‡·', 'æ³•å›½'),
    'IT': ('ğŸ‡®ğŸ‡¹', 'æ„å¤§åˆ©'), 'ES': ('ğŸ‡ªğŸ‡¸', 'è¥¿ç­ç‰™'), 'NL': ('ğŸ‡³ğŸ‡±', 'è·å…°'),
    'FI': ('ğŸ‡«ğŸ‡®', 'èŠ¬å…°'), 'AU': ('ğŸ‡¦ğŸ‡º', 'æ¾³å¤§åˆ©äºš'), 'CA': ('ğŸ‡¨ğŸ‡¦', 'åŠ æ‹¿å¤§'),
    'NZ': ('ğŸ‡³ğŸ‡¿', 'æ–°è¥¿å…°'), 'BR': ('ğŸ‡§ğŸ‡·', 'å·´è¥¿'), 'RU': ('ğŸ‡·ğŸ‡º', 'ä¿„ç½—æ–¯'),
    'PL': ('ğŸ‡µğŸ‡±', 'æ³¢å…°'), 'UA': ('ğŸ‡ºğŸ‡¦', 'ä¹Œå…‹å…°'), 'CZ': ('ğŸ‡¨ğŸ‡¿', 'æ·å…‹'),
    'HU': ('ğŸ‡­ğŸ‡º', 'åŒˆç‰™åˆ©'), 'RO': ('ğŸ‡·ğŸ‡´', 'ç½—é©¬å°¼äºš'), 'SA': ('ğŸ‡¸ğŸ‡¦', 'æ²™ç‰¹é˜¿æ‹‰ä¼¯'),
    'AE': ('ğŸ‡¦ğŸ‡ª', 'é˜¿è”é…‹'), 'QA': ('ğŸ‡¶ğŸ‡¦', 'å¡å¡”å°”'), 'IL': ('ğŸ‡®ğŸ‡±', 'ä»¥è‰²åˆ—'),
    'TR': ('ğŸ‡¹ğŸ‡·', 'åœŸè€³å…¶'), 'IR': ('ğŸ‡®ğŸ‡·', 'ä¼Šæœ—'),
    'CN': ('ğŸ‡¨ğŸ‡³', 'ä¸­å›½'), 'BD': ('ğŸ‡§ğŸ‡©', 'å­ŸåŠ æ‹‰å›½'), 'PK': ('ğŸ‡µğŸ‡°', 'å·´åŸºæ–¯å¦'),
    'LK': ('ğŸ‡±ğŸ‡°', 'æ–¯é‡Œå…°å¡'), 'NP': ('ğŸ‡³ğŸ‡µ', 'å°¼æ³Šå°”'), 'BT': ('ğŸ‡§ğŸ‡¹', 'ä¸ä¸¹'),
    'MV': ('ğŸ‡²ğŸ‡»', 'é©¬å°”ä»£å¤«'), 'BN': ('ğŸ‡§ğŸ‡³', 'æ–‡è±'), 'TL': ('ğŸ‡¹ğŸ‡±', 'ä¸œå¸æ±¶'),
    'EG': ('ğŸ‡ªğŸ‡¬', 'åŸƒåŠ'), 'ZA': ('ğŸ‡¿ğŸ‡¦', 'å—é'), 'NG': ('ğŸ‡³ğŸ‡¬', 'å°¼æ—¥åˆ©äºš'),
    'KE': ('ğŸ‡°ğŸ‡ª', 'è‚¯å°¼äºš'), 'GH': ('ğŸ‡¬ğŸ‡­', 'åŠ çº³'), 'MA': ('ğŸ‡²ğŸ‡¦', 'æ‘©æ´›å“¥'),
    'DZ': ('ğŸ‡©ğŸ‡¿', 'é˜¿å°”åŠåˆ©äºš'), 'TN': ('ğŸ‡¹ğŸ‡³', 'çªå°¼æ–¯'), 'AR': ('ğŸ‡¦ğŸ‡·', 'é˜¿æ ¹å»·'),
    'CL': ('ğŸ‡¨ğŸ‡±', 'æ™ºåˆ©'), 'CO': ('ğŸ‡¨ğŸ‡´', 'å“¥ä¼¦æ¯”äºš'), 'PE': ('ğŸ‡µğŸ‡ª', 'ç§˜é²'),
    'MX': ('ğŸ‡²ğŸ‡½', 'å¢¨è¥¿å“¥'), 'VE': ('ğŸ‡»ğŸ‡ª', 'å§”å†…ç‘æ‹‰'), 'SE': ('ğŸ‡¸ğŸ‡ª', 'ç‘å…¸'),
    'NO': ('ğŸ‡³ğŸ‡´', 'æŒªå¨'), 'DK': ('ğŸ‡©ğŸ‡°', 'ä¸¹éº¦'), 'CH': ('ğŸ‡¨ğŸ‡­', 'ç‘å£«'),
    'AT': ('ğŸ‡¦ğŸ‡¹', 'å¥¥åœ°åˆ©'), 'BE': ('ğŸ‡§ğŸ‡ª', 'æ¯”åˆ©æ—¶'), 'IE': ('ğŸ‡®ğŸ‡ª', 'çˆ±å°”å…°'),
    'PT': ('ğŸ‡µğŸ‡¹', 'è‘¡è„ç‰™'), 'GR': ('ğŸ‡¬ğŸ‡·', 'å¸Œè…Š'), 'BG': ('ğŸ‡§ğŸ‡¬', 'ä¿åŠ åˆ©äºš'),
    'SK': ('ğŸ‡¸ğŸ‡°', 'æ–¯æ´›ä¼å…‹'), 'SI': ('ğŸ‡¸ğŸ‡®', 'æ–¯æ´›æ–‡å°¼äºš'), 'HR': ('ğŸ‡­ğŸ‡·', 'å…‹ç½—åœ°äºš'),
    'RS': ('ğŸ‡·ğŸ‡¸', 'å¡å°”ç»´äºš'), 'BA': ('ğŸ‡§ğŸ‡¦', 'æ³¢é»‘'), 'MK': ('ğŸ‡²ğŸ‡°', 'åŒ—é©¬å…¶é¡¿'),
    'AL': ('ğŸ‡¦ğŸ‡±', 'é˜¿å°”å·´å°¼äºš'), 'KZ': ('ğŸ‡°ğŸ‡¿', 'å“ˆè¨å…‹æ–¯å¦'), 'UZ': ('ğŸ‡ºğŸ‡¿', 'ä¹Œå…¹åˆ«å…‹æ–¯å¦'),
    'KG': ('ğŸ‡°ğŸ‡¬', 'å‰å°”å‰æ–¯æ–¯å¦'), 'TJ': ('ğŸ‡¹ğŸ‡¯', 'å¡”å‰å…‹æ–¯å¦'), 'TM': ('ğŸ‡¹ğŸ‡²', 'åœŸåº“æ›¼æ–¯å¦'),
    'GE': ('ğŸ‡¬ğŸ‡ª', 'æ ¼é²å‰äºš'), 'AM': ('ğŸ‡¦ğŸ‡²', 'äºšç¾å°¼äºš'), 'AZ': ('ğŸ‡¦ğŸ‡¿', 'é˜¿å¡æ‹œç–†'),
    'KW': ('ğŸ‡°ğŸ‡¼', 'ç§‘å¨ç‰¹'), 'BH': ('ğŸ‡§ğŸ‡­', 'å·´æ—'), 'OM': ('ğŸ‡´ğŸ‡²', 'é˜¿æ›¼'),
    'JO': ('ğŸ‡¯ğŸ‡´', 'çº¦æ—¦'), 'LB': ('ğŸ‡±ğŸ‡§', 'é»å·´å«©'), 'SY': ('ğŸ‡¸ğŸ‡¾', 'å™åˆ©äºš'),
    'IQ': ('ğŸ‡®ğŸ‡¶', 'ä¼Šæ‹‰å…‹'), 'YE': ('ğŸ‡¾ğŸ‡ª', 'ä¹Ÿé—¨'),
    'EE': ('ğŸ‡ªğŸ‡ª', 'çˆ±æ²™å°¼äºš'), 'LV': ('ğŸ‡±ğŸ‡»', 'æ‹‰è„±ç»´äºš'), 'LT': ('ğŸ‡±ğŸ‡¹', 'ç«‹é™¶å®›'),
    'MD': ('ğŸ‡²ğŸ‡©', 'æ‘©å°”å¤šç“¦'), 'LU': ('ğŸ‡±ğŸ‡º', 'å¢æ£®å ¡'), 'SC': ('ğŸ‡¸ğŸ‡¨', 'å¡èˆŒå°”'),
    'CY': ('ğŸ‡¨ğŸ‡¾', 'å¡æµ¦è·¯æ–¯'), 'GI': ('ğŸ‡¬ğŸ‡®', 'ç›´å¸ƒç½—é™€'),
}

# å›½å®¶åˆ«åï¼ˆä¿æŒä¸Aè„šæœ¬ä¸€è‡´ï¼‰
COUNTRY_ALIASES = {
    'SOUTH KOREA': 'KR', 'KOREA': 'KR', 'REPUBLIC OF KOREA': 'KR', 'KOREA, REPUBLIC OF': 'KR',
    'HONG KONG': 'HK', 'HONGKONG': 'HK', 'HK SAR': 'HK',
    'UNITED STATES': 'US', 'USA': 'US', 'U.S.': 'US', 'UNITED STATES OF AMERICA': 'US',
    'UNITED KINGDOM': 'GB', 'UK': 'GB', 'GREAT BRITAIN': 'GB', 'è‹±å›½': 'GB',
    'JAPAN': 'JP', 'JPN': 'JP', 'æ—¥æœ¬': 'JP',
    'TAIWAN': 'TW', 'TWN': 'TW', 'TAIWAN, PROVINCE OF CHINA': 'TW', 'å°æ¹¾': 'TW',
    'SINGAPORE': 'SG', 'SGP': 'SG', 'æ–°åŠ å¡': 'SG',
    'FRANCE': 'FR', 'FRA': 'FR', 'æ³•å›½': 'FR',
    'GERMANY': 'DE', 'DEU': 'DE', 'å¾·å›½': 'DE',
    'NETHERLANDS': 'NL', 'NLD': 'NL', 'è·å…°': 'NL',
    'AUSTRALIA': 'AU', 'AUS': 'AU', 'æ¾³å¤§åˆ©äºš': 'AU',
    'CANADA': 'CA', 'CAN': 'CA', 'åŠ æ‹¿å¤§': 'CA',
    'BRAZIL': 'BR', 'BRA': 'BR', 'å·´è¥¿': 'BR',
    'RUSSIA': 'RU', 'RUS': 'RU', 'ä¿„ç½—æ–¯': 'RU',
    'INDIA': 'IN', 'IND': 'IN', 'å°åº¦': 'IN',
    'CHINA': 'CN', 'CHN': 'CN', 'ä¸­å›½': 'CN',
    'VIET NAM': 'VN', 'VIETNAM': 'VN', 'è¶Šå—': 'VN',
    'THAILAND': 'TH', 'THA': 'TH', 'æ³°å›½': 'TH',
    'BURMA': 'MM', 'MYANMAR': 'MM', 'ç¼…ç”¸': 'MM',
    'NORTH KOREA': 'KP', 'KOREA, DEMOCRATIC PEOPLE\'S REPUBLIC OF': 'KP', 'æœé²œ': 'KP',
    'MOLDOVA': 'MD', 'REPUBLIC OF MOLDOVA': 'MD', 'MOLDOVA, REPUBLIC OF': 'MD', 'æ‘©å°”å¤šç“¦': 'MD',
    'LUXEMBOURG': 'LU', 'GRAND DUCHY OF LUXEMBOURG': 'LU', 'å¢æ£®å ¡': 'LU',
    'SEYCHELLES': 'SC', 'REPUBLIC OF SEYCHELLES': 'SC', 'å¡èˆŒå°”': 'SC',
    'CYPRUS': 'CY', 'REPUBLIC OF CYPRUS': 'CY', 'å¡æµ¦è·¯æ–¯': 'CY',
    'GIBRALTAR': 'GI', 'ç›´å¸ƒç½—é™€': 'GI',
}

# åŸå¸‚åˆ°å›½å®¶ä»£ç æ˜ å°„è¡¨ï¼ˆä¿æŒä¸Aè„šæœ¬ä¸€è‡´ï¼‰
CITY_TO_COUNTRY = {
    'TOKYO': 'JP',
    'HONG KONG': 'HK',
    'HONGKONG': 'HK',
    'LOS ANGELES': 'US',
    'MANILA': 'PH',
    'SINGAPORE': 'SG',
    'SAN JOSE': 'US',
    'YEREVAN': 'AM',
    'FRANKFURT': 'DE',
    'AMSTERDAM': 'NL',
    'MOSCOW': 'RU',
    'SEOUL': 'KR',
    'TAIPEI': 'TW',
    'BANGKOK': 'TH',
    'JAKARTA': 'ID',
    'HO CHI MINH CITY': 'VN',
    'HANOI': 'VN',
    'NEW DELHI': 'IN',
    'YANGON': 'MM',
    'MACAU': 'MO',
    'PHNOM PENH': 'KH',
    'VIENTIANE': 'LA',
    'ULAANBAATAR': 'MN',
    'PYONGYANG': 'KP',
    'CHISINAU': 'MD',
    'KISHINEV': 'MD',
    'LUXEMBOURG': 'LU',
    'VICTORIA': 'SC',
    'NICOSIA': 'CY',
    'GIBRALTAR': 'GI',
}

# IATA ä»£ç åˆ°å›½å®¶ä»£ç æ˜ å°„è¡¨ï¼ˆä¿æŒä¸Aè„šæœ¬ä¸€è‡´ï¼‰
IATA_TO_COUNTRY = {
    'NRT': 'JP',
    'HKG': 'HK',
    'LAX': 'US',
    'MNL': 'PH',
    'SIN': 'SG',
    'SJC': 'US',
    'EVN': 'AM',
    'FRA': 'DE',
    'AMS': 'NL',
    'DME': 'RU',
    'ICN': 'KR',
    'TPE': 'TW',
    'BKK': 'TH',
    'CGK': 'ID',
    'SGN': 'VN',
    'HAN': 'VN',
    'DEL': 'IN',
    'RGN': 'MM',
    'MFM': 'MO',
    'PNH': 'KH',
    'VTE': 'LA',
    'ULN': 'MN',
    'KIV': 'MD',
    'LUX': 'LU',
    'SEZ': 'SC',
    'LCA': 'CY',
    'PFO': 'CY',
    'GIB': 'GI',
}

def find_speedtest_script() -> str:
    system = platform.system().lower()
    candidates = []
    if system == "windows":
        candidates = ["iptest.bat", ".\\iptest.bat"]
    else:
        candidates = ["iptest.sh", "./iptest.sh", "iptest", "./iptest"]
    for candidate in candidates:
        if os.path.exists(candidate):
            if not os.access(candidate, os.X_OK) and system != "windows":
                try:
                    os.chmod(candidate, 0o755)
                    logger.info(f"å·²ä¸º {candidate} æ·»åŠ æ‰§è¡Œæƒé™")
                except Exception as e:
                    logger.error(f"æ— æ³•ä¸º {candidate} æ·»åŠ æ‰§è¡Œæƒé™: {e}")
                    continue
            logger.info(f"æ‰¾åˆ°æµ‹é€Ÿè„šæœ¬: {candidate}")
            return candidate
    logger.error("æœªæ‰¾åˆ°æµ‹é€Ÿè„šæœ¬ï¼Œè¯·ç¡®ä¿ iptest.sh æˆ– iptest.bat å­˜åœ¨")
    sys.exit(1)

SPEEDTEST_SCRIPT = find_speedtest_script()

def is_termux() -> bool:
    """æ£€æŸ¥æ˜¯å¦è¿è¡Œåœ¨ Termux ç¯å¢ƒä¸­"""
    return os.getenv("TERMUX_VERSION") is not None or "com.termux" in os.getenv("PREFIX", "")

def parse_speedlimit_from_script(script_path: str) -> float:
    """ä» iptest.sh æˆ– iptest.bat è§£æ speedlimit å‚æ•°ï¼Œé»˜è®¤ä¸º 8.0 MB/s"""
    try:
        # ä½¿ç”¨ charset_normalizer æ£€æµ‹æ–‡ä»¶ç¼–ç 
        with open(script_path, "rb") as f:
            raw_data = f.read()
        detected = detect(raw_data)
        encoding = detected.get("encoding", "utf-8") or "utf-8"
        logger.info(f"æ£€æµ‹åˆ° {script_path} çš„ç¼–ç : {encoding}")

        # è§£ç æ–‡ä»¶å†…å®¹
        content = raw_data.decode(encoding, errors="replace")
        logger.debug(f"{script_path} å†…å®¹ï¼ˆå‰ 1000 å­—ç¬¦ï¼‰: {content[:1000]}")

        # åŒ¹é… speedlimit å‚æ•°ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        speedlimit_match = re.search(
            r'(?:--)?speed(?:limit|_limit)\s*[=:\s]\s*"?(\d*\.?\d*)"?\s*(?:MB/s)?',
            content,
            re.IGNORECASE
        )
        if speedlimit_match:
            speedlimit = float(speedlimit_match.group(1))
            logger.info(f"ä» {script_path} è§£æåˆ° speedlimit: {speedlimit} MB/s")
            return speedlimit

        logger.info(f"æœªåœ¨ {script_path} ä¸­æ‰¾åˆ° speedlimit å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 8.0 MB/s")
        return 8.0
    except Exception as e:
        logger.warning(f"æ— æ³•è§£æ {script_path} çš„ speedlimit å‚æ•°: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼ 8.0 MB/s")
        return 8.0

def filter_ip_csv_by_speed(csv_file: str, speed_limit: float):
    """æ ¹æ® speed_limit è¿‡æ»¤ ip.csv ä¸­çš„ä½é€ŸèŠ‚ç‚¹"""
    try:
        temp_file = csv_file + ".tmp"
        with open(csv_file, "r", encoding="utf-8") as f_in, open(temp_file, "w", newline="", encoding="utf-8") as f_out:
            reader = csv.reader(f_in)
            writer = csv.writer(f_out)
            header = next(reader, None)
            if not header:
                logger.error(f"{csv_file} æ²¡æœ‰æœ‰æ•ˆçš„è¡¨å¤´")
                return
            writer.writerow(header)
            speed_col = 9  # ç¬¬ 10 åˆ—æ˜¯â€œä¸‹è½½é€Ÿåº¦MB/sâ€
            filtered_count = 0
            total_count = 0
            for row in reader:
                total_count += 1
                if len(row) > speed_col and row[speed_col].strip():
                    try:
                        speed = float(row[speed_col])
                        if speed >= speed_limit:
                            writer.writerow(row)
                        else:
                            filtered_count += 1
                    except ValueError:
                        filtered_count += 1
                        continue
                else:
                    filtered_count += 1
            logger.info(f"è¿‡æ»¤ {csv_file}: æ€»è®¡ {total_count} ä¸ªèŠ‚ç‚¹ï¼Œè¿‡æ»¤æ‰ {filtered_count} ä¸ªä½é€ŸèŠ‚ç‚¹ï¼ˆé€Ÿåº¦ < {speed_limit} MB/sï¼‰")
        os.replace(temp_file, csv_file)
    except Exception as e:
        logger.error(f"è¿‡æ»¤ {csv_file} å¤±è´¥: {e}")

geoip_reader = None

def cleanup_temp_file():
    temp_dir = tempfile.gettempdir()
    for temp_file in Path(temp_dir).glob("temp_proxy_*.csv"):
        try:
            temp_file.unlink()
            logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")
        except Exception as e:
            logger.warning(f"æ— æ³•æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {temp_file}: {e}")

atexit.register(cleanup_temp_file)

def setup_and_activate_venv():
    logger = logging.getLogger(__name__)
    
    # å®šä¹‰ä¾èµ–åˆ—è¡¨
    REQUIRED_PACKAGES = ['requests', 'charset-normalizer', 'geoip2==4.8.0', 'maxminddb>=2.0.0', 'packaging>=21.3', 'bs4']  # æ–°å¢bs4
    
    # æ£€æµ‹å¹³å°
    system = sys.platform.lower()
    if system.startswith('win'):
        system = 'windows'
    elif system.startswith('linux'):
        system = 'linux'
    elif system.startswith('darwin'):
        system = 'darwin'
    else:
        logger.error(f"ä¸æ”¯æŒçš„å¹³å°: {system}")
        sys.exit(1)
    
    logger.debug(f"æ£€æµ‹åˆ°çš„å¹³å°: {system}")
    logger.debug(f"Python å¯æ‰§è¡Œæ–‡ä»¶: {sys.executable}, ç‰ˆæœ¬: {sys.version}")
    
    venv_path = Path('.venv')
    logger.debug(f"è™šæ‹Ÿç¯å¢ƒè·¯å¾„: {venv_path}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºè™šæ‹Ÿç¯å¢ƒ
    recreate_venv = False
    if venv_path.exists():
        logger.debug(f"æ£€æµ‹åˆ°ç°æœ‰è™šæ‹Ÿç¯å¢ƒ: {venv_path}")
        venv_python = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'python')
        try:
            result = subprocess.run([venv_python, '--version'], check=True, capture_output=True, text=True)
            logger.debug(f"è™šæ‹Ÿç¯å¢ƒ Python ç‰ˆæœ¬: {result.stdout.strip()}")
        except subprocess.CalledProcessError as e:
            logger.warning(f"è™šæ‹Ÿç¯å¢ƒ Python ä¸å¯ç”¨: {e}, å°†é‡æ–°åˆ›å»º")
            recreate_venv = True
    else:
        logger.debug("æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒï¼Œå°†åˆ›å»º")
        recreate_venv = True
    
    pip_venv = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'pip')
    logger.debug("å¼€å§‹æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒä¾èµ–")
    
    # æ£€æŸ¥å·²å®‰è£…çš„ä¾èµ–
    installed_packages = {}
    if not recreate_venv:
        try:
            result = subprocess.run([pip_venv, "list", "--format=json"], check=True, capture_output=True, text=True)
            logger.debug(f"pip list è¾“å‡º: {result.stdout}")
            installed_packages = {pkg["name"].lower(): pkg["version"] for pkg in json.loads(result.stdout)}
            logger.debug(f"å·²å®‰è£…çš„åŒ…: {installed_packages}")
        except subprocess.CalledProcessError as e:
            logger.error(f"pip list å¤±è´¥: {e}, è¾“å‡º: {e.output}")
            recreate_venv = True
    
    # éªŒè¯ä¾èµ–æ˜¯å¦æ»¡è¶³
    missing_packages = []
    if not recreate_venv:
        for pkg in REQUIRED_PACKAGES:
            if '==' in pkg:
                pkg_name, expected_version = pkg.split('==')
                version_op = '=='
            elif '>=' in pkg:
                pkg_name, expected_version = pkg.split('>=')
                version_op = '>='
            else:
                pkg_name, expected_version = pkg, None
                version_op = None
            pkg_name = pkg_name.lower().replace('_', '-')
            
            if pkg_name not in installed_packages:
                logger.warning(f"æœªæ‰¾åˆ°ä¾èµ–: {pkg_name}")
                missing_packages.append(pkg)
                continue
            
            if expected_version:
                installed_version = installed_packages[pkg_name]
                if version_op == '==' and installed_version != expected_version:
                    logger.warning(f"ä¾èµ– {pkg_name} ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œå®é™… {installed_version}ï¼ŒæœŸæœ› == {expected_version}")
                    missing_packages.append(pkg)
                elif version_op == '>=' and version.parse(installed_version) < version.parse(expected_version):
                    logger.warning(f"ä¾èµ– {pkg_name} ç‰ˆæœ¬è¿‡ä½ï¼Œå®é™… {installed_version}ï¼ŒæœŸæœ› >= {expected_version}")
                    missing_packages.append(pkg)
    
    if missing_packages:
        logger.warning(f"è™šæ‹Ÿç¯å¢ƒç¼ºå°‘ä¾èµ–: {missing_packages}ï¼Œå°†é‡æ–°åˆ›å»º")
        recreate_venv = True
    else:
        logger.info("æ‰€æœ‰ä¾èµ–å·²æ»¡è¶³ï¼Œæ— éœ€é‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ")
        recreate_venv = False
    
    # åˆ›å»ºæˆ–é‡å»ºè™šæ‹Ÿç¯å¢ƒ
    if recreate_venv:
        if venv_path.exists():
            logger.debug("åˆ é™¤ç°æœ‰è™šæ‹Ÿç¯å¢ƒ")
            shutil.rmtree(venv_path, ignore_errors=True)
            logger.debug("æˆåŠŸåˆ é™¤ç°æœ‰è™šæ‹Ÿç¯å¢ƒ")
        
        logger.debug(f"åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ: {venv_path}")
        try:
            subprocess.run([sys.executable, '-m', 'venv', str(venv_path)], check=True)
            logger.debug("è™šæ‹Ÿç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        except subprocess.CalledProcessError as e:
            logger.error(f"åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¤±è´¥: {e}")
            sys.exit(1)
        
        venv_python = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'python')
        pip_venv = str(venv_path / ('Scripts' if system == 'windows' else 'bin') / 'pip')
        logger.debug(f"è™šæ‹Ÿç¯å¢ƒ Python: {venv_python}, pip: {pip_venv}")
        
        # å°è¯•å‡çº§ pipï¼ˆéè‡´å‘½ï¼‰
        try:
            result = subprocess.run([pip_venv, 'install', '--upgrade', 'pip'], check=True, capture_output=True, text=True)
            logger.debug(f"å‡çº§ pip æˆåŠŸ: {result.stdout}")
        except subprocess.CalledProcessError as e:
            logger.warning(f"å‡çº§ pip å¤±è´¥: {e}, è¾“å‡º: {e.output}, ç»§ç»­å®‰è£…ä¾èµ–")
        
        # å®‰è£…ä¾èµ–
        for pkg in REQUIRED_PACKAGES:
            logger.debug(f"å®‰è£…ä¾èµ–: {pkg}")
            try:
                result = subprocess.run([pip_venv, 'install', pkg], check=True, capture_output=True, text=True)
                logger.debug(f"æˆåŠŸå®‰è£…ä¾èµ–: {pkg}, è¾“å‡º: {result.stdout}")
            except subprocess.CalledProcessError as e:
                logger.error(f"å®‰è£…ä¾èµ– {pkg} å¤±è´¥: {e}, è¾“å‡º: {e.output}")
                sys.exit(1)
    
    # å°†è™šæ‹Ÿç¯å¢ƒçš„ site-packages æ·»åŠ åˆ° sys.path
    venv_site = str(venv_path / ('Lib' if system == 'windows' else 'lib') / 
                    f"python{sys.version_info.major}.{sys.version_info.minor}" / 'site-packages')
    logger.debug(f"è™šæ‹Ÿç¯å¢ƒ site-packages: {venv_site}")
    if venv_site not in sys.path:
        sys.path.insert(0, venv_site)
    logger.debug("è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»")
    
    # æ¸…ç†æ¨¡å—ç¼“å­˜
    for module in list(sys.modules.keys()):
        if module.startswith('geoip2') or module.startswith('maxminddb') or module.startswith('bs4'):
            del sys.modules[module]
    logger.debug("å·²æ¸…ç† geoip2ã€maxminddb å’Œ bs4 æ¨¡å—ç¼“å­˜")
    
    # éªŒè¯å…³é”®æ¨¡å—
    try:
        import geoip2.database
        import maxminddb
        import packaging
        import bs4
        logger.debug("æ‰€æœ‰å…³é”®æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥å…³é”®æ¨¡å—: {e}")
        sys.exit(1)

def get_latest_geoip_url() -> str:
    api_url = "https://api.github.com/repos/P3TERX/GeoLite.mmdb/releases/latest"
    logger.info(f"æ­£åœ¨ä» GitHub API è·å–æœ€æ–°ç‰ˆæœ¬: {api_url}")
    try:
        session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504, 429])
        session.mount('https://', HTTPAdapter(max_retries=retry))
        response = session.get(api_url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        release_data = response.json()
        
        for asset in release_data.get("assets", []):
            if asset.get("name") == "GeoLite2-Country.mmdb":
                download_url = asset.get("browser_download_url")
                logger.info(f"æ‰¾åˆ°æœ€æ–° GeoIP æ•°æ®åº“ URL: {download_url}")
                return download_url
        
        logger.error("æœªæ‰¾åˆ° GeoLite2-Country.mmdb çš„ä¸‹è½½ URL")
        return ""
    except Exception as e:
        logger.error(f"æ— æ³•è·å–æœ€æ–° GeoIP æ•°æ®åº“ URL: {e}")
        return ""

def download_geoip_database(dest_path: Path) -> bool:
    url = get_latest_geoip_url()
    if not url:
        logger.error("æ— æ³•è·å–æœ€æ–° GeoIP æ•°æ®åº“ URL")
        return False
    
    proxy_services = [
        ("Ghfast.top", "https://ghfast.top/"),
        ("Gitproxy.clickr", "https://gitproxy.click/"),
        ("Gh-proxy.ygxz", "https://gh-proxy.ygxz.in/"),
        ("Github.ur1.fun", "https://github.ur1.fun/")
    ]
    
    urls_to_try = [("æ— ä»£ç†", url)]
    for proxy_name, proxy_prefix in proxy_services:
        if url.startswith("https://github.com/"):
            proxy_url = proxy_prefix + url
            urls_to_try.append((proxy_name, proxy_url))
    
    for proxy_name, download_url in urls_to_try:
        logger.info(f"ä¸‹è½½ GeoIP æ•°æ®åº“ï¼ˆä½¿ç”¨ {proxy_name}ï¼‰: {download_url}")
        try:
            session = requests.Session()
            retry = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504, 429])
            session.mount('https://', HTTPAdapter(max_retries=retry))
            response = session.get(download_url, timeout=60, stream=True, headers=HEADERS)
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            with open(dest_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            logger.info(f"ä¸‹è½½è¿›åº¦: {progress:.2f}%")
            logger.info(f"GeoIP æ•°æ®åº“ä¸‹è½½å®Œæˆ: {dest_path}")
            if not dest_path.exists() or dest_path.stat().st_size < 100:
                logger.error(f"ä¸‹è½½çš„ GeoIP æ•°æ®åº“æ— æ•ˆ")
                dest_path.unlink(missing_ok=True)
                return False
            return True
        except Exception as e:
            logger.warning(f"é€šè¿‡ {proxy_name} ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥: {e}")
            continue
    
    logger.error("æ‰€æœ‰ä»£ç†æœåŠ¡å‡æ— æ³•ä¸‹è½½ GeoIP æ•°æ®åº“")
    return False

def download_geoip_database_maxmind(dest_path: Path) -> bool:
    if not MAXMIND_LICENSE_KEY:
        logger.warning("æœªè®¾ç½® MAXMIND_LICENSE_KEYï¼Œæ— æ³•ä» MaxMind ä¸‹è½½ GeoIP æ•°æ®åº“ã€‚è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® MAXMIND_LICENSE_KEY æˆ–æ£€æŸ¥ GitHub ä¸‹è½½æºã€‚")
        return False
    url = GEOIP_DB_URL_BACKUP.format(MAXMIND_LICENSE_KEY)
    logger.info(f"ä» MaxMind ä¸‹è½½ GeoIP æ•°æ®åº“: {url}")
    try:
        if dest_path.exists():
            logger.info(f"åˆ é™¤æ—§çš„ GeoIP æ•°æ®åº“æ–‡ä»¶: {dest_path}")
            dest_path.unlink(missing_ok=True)
            
        session = requests.Session()
        retry = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504, 429])
        session.mount('https://', HTTPAdapter(max_retries=retry))
        response = session.get(url, timeout=60, stream=True, headers=HEADERS)
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        temp_tar = dest_path.with_suffix(".tar.gz")
        with open(temp_tar, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        logger.info(f"ä¸‹è½½è¿›åº¦: {progress:.2f}%")
        with tarfile.open(temp_tar, "r:gz") as tar:
            for member in tar.getmembers():
                if member.name.endswith("GeoLite2-Country.mmdb"):
                    tar.extract(member, dest_path.parent)
                    extracted_path = dest_path.parent / member.name
                    extracted_path.rename(dest_path)
                    break
        temp_tar.unlink(missing_ok=True)
        if not dest_path.exists() or dest_path.stat().st_size < 100:
            logger.error(f"è§£å‹çš„ GeoIP æ•°æ®åº“æ— æ•ˆ")
            dest_path.unlink(missing_ok=True)
            return False
        return True
    except Exception as e:
        logger.error(f"ä» MaxMind ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥: {e}")
        temp_tar.unlink(missing_ok=True)
        return False

def init_geoip_reader(offline: bool = False, update_geoip: bool = False):
    global geoip_reader
    
    def is_geoip_file_valid(file_path: Path) -> bool:
        if not file_path.exists():
            return False
        if file_path.stat().st_size < 1024 * 1024:
            logger.warning(f"GeoIP æ•°æ®åº“æ–‡ä»¶ {file_path} è¿‡å°ï¼Œå¯èƒ½æ— æ•ˆ")
            return False
        mtime = file_path.stat().st_mtime
        current_time = time.time()
        age_days = (current_time - mtime) / (24 * 3600)
        if age_days > 30:
            logger.warning(f"GeoIP æ•°æ®åº“æ–‡ä»¶ {file_path} å·²è¶…è¿‡ 30 å¤© ({age_days:.1f} å¤©)ï¼Œå»ºè®®ä½¿ç”¨ --update-geoip æ›´æ–°")
        return True
    
    if offline:
        logger.info("ç¦»çº¿æ¨¡å¼å¯ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ° GeoIP æ•°æ®åº“")
        if not GEOIP_DB_PATH.exists():
            logger.error(f"ç¦»çº¿æ¨¡å¼ä¸‹æœªæ‰¾åˆ°æœ¬åœ° GeoIP æ•°æ®åº“: {GEOIP_DB_PATH}")
            sys.exit(1)
    else:
        if update_geoip:
            logger.info("æ£€æµ‹åˆ° --update-geoip å‚æ•°ï¼Œå¼ºåˆ¶æ›´æ–° GeoIP æ•°æ®åº“")
            GEOIP_DB_PATH.unlink(missing_ok=True)
        if GEOIP_DB_PATH.exists() and is_geoip_file_valid(GEOIP_DB_PATH):
            logger.info(f"æœ¬åœ° GeoIP æ•°æ®åº“å·²å­˜åœ¨ä¸”æœ‰æ•ˆ: {GEOIP_DB_PATH}ï¼Œç›´æ¥ä½¿ç”¨")
        else:
            if GEOIP_DB_PATH.exists():
                logger.info(f"æœ¬åœ° GeoIP æ•°æ®åº“æ— æ•ˆ: {GEOIP_DB_PATH}ï¼Œå°†é‡æ–°ä¸‹è½½")
                GEOIP_DB_PATH.unlink(missing_ok=True)
            else:
                logger.info(f"æœ¬åœ° GeoIP æ•°æ®åº“ä¸å­˜åœ¨: {GEOIP_DB_PATH}ï¼Œå°è¯•ä¸‹è½½æœ€æ–°æ–‡ä»¶")
            success = download_geoip_database(GEOIP_DB_PATH)
            if not success:
                logger.warning("ä¸»ä¸‹è½½æºå¤±è´¥ï¼Œå°è¯• MaxMind")
                success = download_geoip_database_maxmind(GEOIP_DB_PATH)
                if not success:
                    logger.error("ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥ï¼Œä¸”æœ¬åœ°æ— å¯ç”¨æ•°æ®åº“")
                    sys.exit(1)
    
    try:
        import geoip2.database
        with geoip2.database.Reader(GEOIP_DB_PATH) as reader:
            logger.info("GeoIP æ•°æ®åº“éªŒè¯æˆåŠŸ")
        geoip_reader = geoip2.database.Reader(GEOIP_DB_PATH)
        logger.info("GeoIP æ•°æ®åº“åŠ è½½æˆåŠŸ")
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥ geoip2.database: {e}. è¯·ç¡®ä¿ geoip2==4.8.0 å·²å®‰è£…ï¼Œå¹¶æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"GeoIP æ•°æ®åº“åŠ è½½å¤±è´¥: {e}, ç±»å‹: {type(e).__name__}")
        if offline:
            logger.error("ç¦»çº¿æ¨¡å¼ä¸‹æ— æ³•åŠ è½½ GeoIP æ•°æ®åº“ï¼Œé€€å‡º")
            sys.exit(1)
        logger.info("æœ¬åœ°æ•°æ®åº“å¯èƒ½æŸåï¼Œå°è¯•é‡æ–°ä¸‹è½½ GeoIP æ•°æ®åº“")
        GEOIP_DB_PATH.unlink(missing_ok=True)
        success = download_geoip_database(GEOIP_DB_PATH)
        if not success:
            logger.warning("ä¸»ä¸‹è½½æºå¤±è´¥ï¼Œå°è¯• MaxMind")
            success = download_geoip_database_maxmind(GEOIP_DB_PATH)
            if not success:
                logger.error("é‡æ–°ä¸‹è½½ GeoIP æ•°æ®åº“å¤±è´¥")
                sys.exit(1)
        with geoip2.database.Reader(GEOIP_DB_PATH) as reader:
            logger.info("GeoIP æ•°æ®åº“éªŒè¯æˆåŠŸ")
        geoip_reader = geoip2.database.Reader(GEOIP_DB_PATH)
        logger.info("GeoIP æ•°æ®åº“åŠ è½½æˆåŠŸ")

def close_geoip_reader():
    global geoip_reader
    if geoip_reader:
        try:
            geoip_reader.close()
            logger.info("GeoIP æ•°æ®åº“å·²å…³é—­")
        except Exception as e:
            logger.warning(f"å…³é—­ GeoIP æ•°æ®åº“å¤±è´¥: {e}")
        geoip_reader = None

atexit.register(close_geoip_reader)

def check_dependencies(offline: bool = False, update_geoip: bool = False):
    init_geoip_reader(offline=offline, update_geoip=update_geoip)

def load_country_cache() -> Dict[str, str]:
    if os.path.exists(COUNTRY_CACHE_FILE):
        try:
            with open(COUNTRY_CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½å›½å®¶ç¼“å­˜: {e}")
    return {}

def save_country_cache(cache: Dict[str, str]):
    try:
        with open(COUNTRY_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"æ— æ³•ä¿å­˜å›½å®¶ç¼“å­˜: {e}")

def is_temp_file_valid(temp_file: str) -> bool:
    if not os.path.exists(temp_file):
        return False
    mtime = os.path.getmtime(temp_file)
    current_time = time.time()
    if (current_time - mtime) > TEMP_FILE_CACHE_DURATION:
        logger.info(f"ä¸´æ—¶æ–‡ä»¶ {temp_file} å·²è¿‡æœŸ")
        return False
    if os.path.getsize(temp_file) < 10:
        logger.warning(f"ä¸´æ—¶æ–‡ä»¶ {temp_file} å†…å®¹å¤ªå°")
        return False
    return True

def detect_delimiter(lines: List[str]) -> str:
    sample_lines = lines[:5]
    delimiters = [',', ';', '\t', ' ', '|', '-']
    counts = {d: 0 for d in delimiters}
    for line in sample_lines:
        if not line.strip() or line.startswith('#'):
            continue
        for d in delimiters:
            if d in line:
                counts[d] += line.count(d)
    max_count = max(counts.values())
    if max_count > 0:
        delimiter = max(counts, key=counts.get)
        logger.info(f"æ£€æµ‹åˆ°åˆ†éš”ç¬¦: '{delimiter}'")
        return delimiter
    logger.warning("æ— æ³•æ£€æµ‹åˆ†éš”ç¬¦ï¼Œå‡å®šä¸ºé€—å·")
    return ','

def is_valid_ip(ip: str) -> bool:
    ipv4_pattern = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')
    ipv6_pattern = re.compile(r'^(?:[0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}$')
    return bool(ipv4_pattern.match(ip) or ipv6_pattern.match(ip.strip('[]')))

def is_valid_port(port: str) -> bool:
    try:
        port_num = int(port)
        return 0 <= port_num <= 65535
    except (ValueError, TypeError):
        return False

def is_country_like(value: str) -> bool:
    if not value:
        return False
    value_upper = value.upper().strip()
    if re.match(r'^[A-Z]{2}$', value_upper) and value_upper in COUNTRY_LABELS:
        return True
    if value_upper in COUNTRY_ALIASES:
        return True
    return False

def standardize_country(value: str) -> str:
    if not value:
        return ''
    value_clean = re.sub(r'[^a-zA-Z\s]', '', value).strip().upper()
    
    if value_clean in COUNTRY_LABELS:
        return value_clean
    
    if value_clean in COUNTRY_ALIASES:
        return COUNTRY_ALIASES[value_clean]
    
    if value_clean in CITY_TO_COUNTRY:
        return CITY_TO_COUNTRY[value_clean]
    
    if value_clean in IATA_TO_COUNTRY:
        return IATA_TO_COUNTRY[value_clean]
    
    value_no_space = value_clean.replace(' ', '')
    for alias, code in COUNTRY_ALIASES.items():
        alias_clean = alias.replace(' ', '')
        if value_no_space == alias_clean:
            return code
    for city, code in CITY_TO_COUNTRY.items():
        city_clean = city.replace(' ', '')
        if value_no_space == city_clean:
            return code
    
    return ''

def find_country_column(lines: List[str], delimiter: str) -> Tuple[int, str, int]:
    country_col = -1
    ip_col, port_col = 0, 1
    sample_lines = [line for line in lines[:20] if line.strip() and not line.startswith('#')]
    if not sample_lines:
        return ip_col, port_col, country_col

    col_matches = defaultdict(int)
    total_rows = len(sample_lines)
    for line in sample_lines:
        fields = line.split(delimiter)
        for col, field in enumerate(fields):
            field = field.strip()
            standardized = standardize_country(field)
            if standardized:
                col_matches[col] += 1

    if col_matches:
        for col, count in col_matches.items():
            logger.info(f"åˆ— {col + 1}: åŒ¹é… {count} è¡Œ (åŒ¹é…ç‡: {count / total_rows:.2%})")
        country_col = max(col_matches, key=col_matches.get)
        match_rate = col_matches[country_col] / total_rows
        if match_rate >= 0.3:
            logger.info(f"é€‰æ‹©å›½å®¶åˆ—: ç¬¬ {country_col + 1} åˆ— (åŒ¹é…ç‡: {match_rate:.2%})")
        else:
            country_col = -1
    else:
        logger.info("æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…å›½å®¶ä»£ç ã€åŸå¸‚æˆ– IATA ä»£ç åˆ—")

    return ip_col, port_col, country_col

def fetch_and_save_to_temp_file(url: str) -> str:
    logger.info(f"ä¸‹è½½ URL: {url} åˆ° {TEMP_FILE}")
    try:
        session = requests.Session()
        retry = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504, 429])
        session.mount('https://', HTTPAdapter(max_retries=retry))
        response = session.get(url, timeout=60, headers=HEADERS, stream=True)
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        temp_content = []
        with open(TEMP_FILE, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    temp_content.append(chunk)
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        logger.info(f"ä¸‹è½½è¿›åº¦: {progress:.2f}%")
        
        try:
            with open(TEMP_FILE, "rb") as f:
                raw_data = f.read()
            encoding = detect(raw_data).get("encoding", "utf-8")
            content = raw_data.decode(encoding)
            lines = content.strip().splitlines()
            if not lines:
                logger.error(f"ä¸‹è½½çš„æ–‡ä»¶ {TEMP_FILE} ä¸ºç©º")
                return ''
            logger.info(f"ä¸‹è½½æ–‡ä»¶ç¼–ç : {encoding}")
            logger.info(f"ä¸‹è½½æ–‡ä»¶å‰ 5 è¡Œ: {lines[:5]}")
            delimiter = detect_delimiter(lines)
            if not delimiter:
                logger.error(f"ä¸‹è½½çš„æ–‡ä»¶ {TEMP_FILE} æ— æ³•æ£€æµ‹åˆ†éš”ç¬¦")
                return ''
            header = lines[0].strip().split(delimiter)
            if len(header) < 2 or 'ip' not in header[0].lower():
                logger.warning(f"ä¸‹è½½çš„æ–‡ä»¶ {TEMP_FILE} è¡¨å¤´å¯èƒ½æ— æ•ˆ: {header}")
            if len(lines) < 2:
                logger.error(f"ä¸‹è½½çš„æ–‡ä»¶ {TEMP_FILE} ç¼ºå°‘æ•°æ®è¡Œ")
                return ''
        except Exception as e:
            logger.error(f"éªŒè¯ä¸‹è½½æ–‡ä»¶æ ¼å¼å¤±è´¥: {e}")
            return ''
        
        logger.info(f"å·²ä¸‹è½½åˆ° {TEMP_FILE}")
        return TEMP_FILE
    except Exception as e:
        logger.error(f"æ— æ³•ä¸‹è½½ URL: {e}")
        return ''

def fetch_multiple_urls_to_temp_files(urls: List[str]) -> List[str]:
    """ä»å¤šä¸ª URL ä¸‹è½½å†…å®¹å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œè¿”å›æœ‰æ•ˆä¸´æ—¶æ–‡ä»¶åˆ—è¡¨"""
    temp_files = []
    for idx, url in enumerate(urls):
        temp_file = os.path.join(tempfile.gettempdir(), f"temp_proxy_{idx}.csv")
        logger.info(f"ä¸‹è½½ URL: {url} åˆ° {temp_file}")
        try:
            session = requests.Session()
            retry = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504, 429])
            session.mount('https://', HTTPAdapter(max_retries=retry))
            response = session.get(url, timeout=60, headers=HEADERS, stream=True)
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            with open(temp_file, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            logger.info(f"ä¸‹è½½è¿›åº¦ ({url}): {progress:.2f}%")
            
            # éªŒè¯æ–‡ä»¶
            with open(temp_file, "rb") as f:
                raw_data = f.read()
            encoding = detect(raw_data).get("encoding", "utf-8")
            content = raw_data.decode(encoding)
            lines = content.strip().splitlines()
            if not lines:
                logger.error(f"ä¸‹è½½çš„æ–‡ä»¶ {temp_file} ä¸ºç©º")
                os.remove(temp_file)
                continue
            delimiter = detect_delimiter(lines)
            if not delimiter:
                logger.error(f"ä¸‹è½½çš„æ–‡ä»¶ {temp_file} æ— æ³•æ£€æµ‹åˆ†éš”ç¬¦")
                os.remove(temp_file)
                continue
            temp_files.append(temp_file)
            logger.info(f"å·²ä¸‹è½½åˆ° {temp_file}")
        except Exception as e:
            logger.error(f"æ— æ³•ä¸‹è½½ URL {url}: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
    return temp_files

def fetch_all_sources(args: argparse.Namespace) -> List[Tuple[str, int, str]]:
    """å¹¶è¡Œå¤„ç†æ‰€æœ‰åœ¨çº¿æ¥æºï¼ˆINPUT_URLS å’Œ WEB_URLSï¼‰ï¼Œè¿”å›åˆå¹¶çš„èŠ‚ç‚¹åˆ—è¡¨"""
    ip_ports = []
    futures = []

    with ThreadPoolExecutor(max_workers=4) as executor:
        # å¤„ç† INPUT_URLS
        if args.url and not args.offline:
            logger.info(f"ä» INPUT_URLS å¼€å§‹æå–èŠ‚ç‚¹: {args.url}")
            futures.append(executor.submit(fetch_multiple_urls_to_temp_files, args.url))
        # å¤„ç† WEB_URLS
        if WEB_URLS and not args.offline:
            logger.info(f"ä» WEB_URLS å¼€å§‹æå–èŠ‚ç‚¹: {WEB_URLS}")
            futures.append(executor.submit(extract_ip_ports_from_web, WEB_URLS, WEB_PORTS))

        for future in futures:
            result = future.result()
            if isinstance(result, list) and all(isinstance(item, str) for item in result):  # TEMP_FILES
                for temp_file in result:
                    if is_temp_file_valid(temp_file):
                        temp_ip_ports = extract_ip_ports_from_file(temp_file)
                        ip_ports.extend(temp_ip_ports)
                        logger.info(f"ä»ä¸´æ—¶æ–‡ä»¶ {temp_file} æå–åˆ° {len(temp_ip_ports)} ä¸ªèŠ‚ç‚¹")
                        os.remove(temp_file)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            elif isinstance(result, list):  # WEB_IP_PORTS
                ip_ports.extend(result)
                logger.info(f"ä» WEB_URLS æå–åˆ° {len(result)} ä¸ªèŠ‚ç‚¹")

    return list(dict.fromkeys(ip_ports))  # å»é‡

def extract_ip_ports_from_file(file_path: str) -> List[Tuple[str, int, str]]:
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return []
    start_time = time.time()
    with open(file_path, "rb") as f:
        raw_data = f.read()
    encoding = detect(raw_data).get("encoding", "utf-8")
    logger.info(f"æ–‡ä»¶ {file_path} ç¼–ç : {encoding}")
    try:
        content = raw_data.decode(encoding)
    except UnicodeDecodeError as e:
        logger.error(f"æ— æ³•è§£ç æ–‡ä»¶ {file_path}: {e}")
        return []
    ip_ports = extract_ip_ports_from_content(content)
    logger.info(f"æ–‡ä»¶ {file_path} è§£æå®Œæˆ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    return ip_ports

def extract_ip_ports_from_web(urls: List[str], ports: List[int]) -> List[Tuple[str, int, str]]:
    """ä»æŒ‡å®šç½‘é¡µæå–IPå¹¶åˆ†é…ç«¯å£"""
    ip_ports = []
    ipv4_pattern = re.compile(r'(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)')
    
    for url in urls:
        logger.info(f"æ­£åœ¨ä»ç½‘é¡µæå– IP: {url}")
        try:
            session = requests.Session()
            retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
            session.mount('https://', HTTPAdapter(max_retries=retry))
            response = session.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
            ips = set(ipv4_pattern.findall(text))
            
            logger.info(f"ä» {url} æå–åˆ° {len(ips)} ä¸ªå”¯ä¸€ IP")
            
            for ip in ips:
                if is_valid_ip(ip):
                    for port in ports:
                        ip_ports.append((ip, port, ''))  # å›½å®¶ä¿¡æ¯ç•™ç©ºï¼Œä¾èµ–GeoIPæŸ¥è¯¢
                else:
                    logger.debug(f"æ— æ•ˆ IP åœ°å€: {ip}")
                    
        except Exception as e:
            logger.error(f"æ— æ³•ä» {url} æå– IP: {e}")
            continue
    
    unique_ip_ports = list(dict.fromkeys(ip_ports))
    logger.info(f"ä»ç½‘é¡µå…±æå– {len(unique_ip_ports)} ä¸ªå”¯ä¸€ IP:ç«¯å£å¯¹")
    return unique_ip_ports

def extract_ip_ports_from_content(content: str) -> List[Tuple[str, int, str]]:
    server_port_pairs = []
    invalid_lines = []
    content = content.replace('\r\n', '\n').replace('\r', '\n')
    lines = content.splitlines()
    if not lines:
        logger.error("å†…å®¹ä¸ºç©º")
        return []

    logger.info(f"æ•°æ®æºæ ·æœ¬ (å‰ 5 è¡Œ): {lines[:5]}")

    try:
        data = json.loads(content)
        for item in data:
            ip = item.get('ip', '') or item.get('IP Address', '') or item.get('ip_address', '')
            port = item.get('port', '') or item.get('Port', '')
            country = standardize_country(
                item.get('country', '') or
                item.get('countryCode', '') or
                item.get('country_code', '') or
                item.get('location', '') or
                item.get('nation', '') or
                item.get('region', '') or
                item.get('geo', '') or
                item.get('area', '') or
                item.get('dc city', '') or
                item.get('dc_city', '') or
                item.get('city', '') or
                item.get('dc location', '') or
                item.get('dc_location', '')
            )
            if is_valid_ip(ip) and is_valid_port(str(port)):
                server_port_pairs.append((ip, int(port), country))
        logger.info(f"ä» JSON è§£æå‡º {len(server_port_pairs)} ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä¸­ {sum(1 for _, _, c in server_port_pairs if c)} ä¸ªæœ‰å›½å®¶ä¿¡æ¯")
        return list(dict.fromkeys(server_port_pairs))
    except json.JSONDecodeError as e:
        logger.info(f"JSON è§£æå¤±è´¥: {e}")

    delimiter = detect_delimiter(lines)
    if not delimiter:
        logger.warning("æ— æ³•æ£€æµ‹åˆ†éš”ç¬¦ï¼Œå‡å®šä¸ºé€—å·")
        delimiter = ','

    ip_col, port_col, country_col = 0, 1, -1
    lines_to_process = lines
    if lines and lines[0].strip() and not lines[0].startswith('#'):
        header = lines[0].strip().split(delimiter)
        logger.info(f"æ£€æµ‹åˆ°è¡¨å¤´: {header}")
        for idx, col in enumerate(header):
            col_lower = col.strip().lower()
            if col_lower in ['ip', 'address', 'ip_address', 'ipåœ°å€', 'ip address']:
                ip_col = idx
            elif col_lower in ['port', 'ç«¯å£', 'port_number', 'ç«¯å£å·']:
                port_col = idx
            elif col_lower in ['country', 'å›½å®¶', 'country_code', 'countrycode', 'å›½é™…ä»£ç ', 'nation', 'location', 'region', 'geo', 'area', 'Country', 'cc', 'iso_code', 'country_name', 'dc city', 'dc_city', 'city', 'dc location', 'dc_location']:
                country_col = idx
        if country_col != -1:
            logger.info(f"æ£€æµ‹åˆ°å›½å®¶åˆ—: ç¬¬ {country_col + 1} åˆ— (å­—æ®µå: {header[country_col]})")
            lines_to_process = lines[1:]
        else:
            logger.info("è¡¨å¤´ä¸­ä¸åŒ…å«å›½å®¶ç›¸å…³åˆ—ï¼Œå°è¯•é€è¡Œé€åˆ—æœç´¢")
            ip_col, port_col, country_col = find_country_column(lines, delimiter)
            if country_col >= 0:
                logger.info(f"é€šè¿‡é€è¡Œæœç´¢ç¡®å®šå›½å®¶åˆ—: ç¬¬ {country_col + 1} åˆ—")
            else:
                logger.info(f"æ— æ³•ç¡®å®šå›½å®¶åˆ—ï¼Œè®¾ä¸º -1")

    ip_port_pattern = re.compile(
        r'(((\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})|\[(?:[0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}\]|(?:[0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}))[ :,\t](\d{1,5})'
    )

    for i, line in enumerate(lines_to_process):
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        match = ip_port_pattern.match(line)
        if match:
            server = match.group(1).strip('[]')
            port = match.group(4)
            country = ''
            if delimiter:
                fields = line.split(delimiter)
                if country_col != -1 and country_col < len(fields):
                    country = standardize_country(fields[country_col].strip())
                if not country:
                    for col, field in enumerate(fields):
                        field = field.strip()
                        potential_country = standardize_country(field)
                        if potential_country:
                            country = potential_country
                            break
            if is_valid_port(port):
                server_port_pairs.append((server, int(port), country))
            else:
                invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (ç«¯å£æ— æ•ˆ)")
            continue
        if delimiter:
            fields = line.split(delimiter)
            if len(fields) < max(ip_col, port_col, country_col) + 1:
                invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (å­—æ®µå¤ªå°‘)")
                continue
            server = fields[ip_col].strip('[]')
            port_str = fields[port_col].strip()
            country = ''
            if country_col != -1 and country_col < len(fields):
                country = standardize_country(fields[country_col].strip())
            if not country:
                for col, field in enumerate(fields):
                    field = field.strip()
                    potential_country = standardize_country(field)
                    if potential_country:
                        country = potential_country
                        break
            if is_valid_ip(server) and is_valid_port(port_str):
                server_port_pairs.append((server, int(port_str), country))
            else:
                invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (IP æˆ–ç«¯å£æ— æ•ˆ)")
        else:
            invalid_lines.append(f"ç¬¬ {i} è¡Œ: {line} (æ ¼å¼æ— æ•ˆ)")

    if invalid_lines:
        logger.info(f"å‘ç° {len(invalid_lines)} ä¸ªæ— æ•ˆæ¡ç›®")
    logger.info(f"è§£æå‡º {len(server_port_pairs)} ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä¸­ {sum(1 for _, _, c in server_port_pairs if c)} ä¸ªæœ‰å›½å®¶ä¿¡æ¯")
    unique_server_port_pairs = list(dict.fromkeys(server_port_pairs))
    logger.info(f"å»é‡å: {len(unique_server_port_pairs)} ä¸ªèŠ‚ç‚¹")
    return unique_server_port_pairs

def get_country_from_ip(ip: str, cache: Dict[str, str]) -> str:
    if ip in cache:
        return cache[ip]
    try:
        response = geoip_reader.country(ip)
        country_code = response.country.iso_code or ''
        if country_code:
            cache[ip] = country_code
            return country_code
        return ''
    except Exception:
        return ''

def get_countries_from_ips(ips: List[str], cache: Dict[str, str]) -> List[str]:
    uncached_ips = [ip for ip in ips if ip not in cache]
    if uncached_ips:
        logger.info(f"æ‰¹é‡æŸ¥è¯¢ {len(uncached_ips)} ä¸ª IP çš„å›½å®¶ä¿¡æ¯")
        for ip in uncached_ips:
            try:
                response = geoip_reader.country(ip)
                cache[ip] = response.country.iso_code or ''
            except Exception:
                cache[ip] = ''
    return [cache[ip] for ip in ips]

def write_ip_list(ip_ports: List[Tuple[str, int, str]], is_github_actions: bool) -> str:
    if not ip_ports:
        logger.error(f"æ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹æ¥ç”Ÿæˆ {IP_LIST_FILE}")
        return None

    start_time = time.time()
    country_cache = load_country_cache()
    filtered_ip_ports = set()
    country_counts = defaultdict(int)
    filtered_counts = defaultdict(int)
    logger.info(f"å¼€å§‹å¤„ç† {len(ip_ports)} ä¸ªèŠ‚ç‚¹...")

    from_source = sum(1 for _, _, country in ip_ports if country and country in COUNTRY_LABELS)
    logger.info(f"æ•°æ®æºä¸º {from_source} ä¸ªèŠ‚ç‚¹æä¾›äº†æœ‰æ•ˆå›½å®¶ä¿¡æ¯ï¼ˆåŒ…æ‹¬åŸå¸‚æ˜ å°„ï¼‰")

    # æ”¶é›†éœ€è¦æŸ¥è¯¢æ•°æ®åº“çš„ IPï¼ˆå›½å®¶ä¿¡æ¯ä¸ºç©ºæˆ–æ— æ•ˆï¼‰
    ips_to_query = [ip for ip, _, country in ip_ports if not country or country not in COUNTRY_LABELS]
    supplemented = 0
    if ips_to_query:
        logger.info(f"æ‰¹é‡æŸ¥è¯¢ {len(ips_to_query)} ä¸ª IP çš„å›½å®¶ä¿¡æ¯ï¼ˆç¼ºå¤±æˆ–æ— æ•ˆï¼‰")
        countries = get_countries_from_ips(ips_to_query, country_cache)
        ip_country_map = dict(zip(ips_to_query, countries))
        supplemented = sum(1 for country in countries if country)
    else:
        ip_country_map = {}

    for ip, port, country in ip_ports:
        final_country = country
        source = "æ•°æ®æº" if country and country in COUNTRY_LABELS else "å¾…æŸ¥è¯¢"
        
        if not country or country not in COUNTRY_LABELS:
            final_country = ip_country_map.get(ip, '')
            if final_country:
                source = "GeoIP æ•°æ®åº“"
        
        if not DESIRED_COUNTRIES:
            filtered_ip_ports.add((ip, port))
            if final_country:
                country_counts[final_country] += 1
        elif final_country and final_country in DESIRED_COUNTRIES:
            filtered_ip_ports.add((ip, port))
            country_counts[final_country] += 1
        else:
            filtered_counts[final_country or 'UNKNOWN'] += 1

    total_retained = len(filtered_ip_ports)
    total_filtered = sum(filtered_counts.values())
    logger.info(f"è¿‡æ»¤ç»“æœ: ä¿ç•™ {total_retained} ä¸ªèŠ‚ç‚¹ï¼Œè¿‡æ»¤æ‰ {total_filtered} ä¸ªèŠ‚ç‚¹")
    logger.info(f"é€šè¿‡ GeoIP æ•°æ®åº“è¡¥å……å›½å®¶ä¿¡æ¯: {supplemented} ä¸ªèŠ‚ç‚¹")
    logger.info(f"ä¿ç•™çš„å›½å®¶åˆ†å¸ƒ: {dict(country_counts)}")
    logger.info(f"è¿‡æ»¤æ‰çš„å›½å®¶åˆ†å¸ƒ: {dict(filtered_counts)}")

    if not filtered_ip_ports:
        logger.error(f"æ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹æ¥ç”Ÿæˆ {IP_LIST_FILE}")
        return None

    with open(IP_LIST_FILE, "w", encoding="utf-8") as f:
        for ip, port in filtered_ip_ports:
            f.write(f"{ip} {port}\n")
    logger.info(f"å·²ç”Ÿæˆ {IP_LIST_FILE}")

    logger.info(f"ç”Ÿæˆ {IP_LIST_FILE}ï¼ŒåŒ…å« {len(filtered_ip_ports)} ä¸ªèŠ‚ç‚¹ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    save_country_cache(country_cache)
    return IP_LIST_FILE

def run_speed_test() -> str:
    if not SPEEDTEST_SCRIPT:
        logger.info("æœªæ‰¾åˆ°æµ‹é€Ÿè„šæœ¬")
        return None

    if not os.path.exists(IP_LIST_FILE):
        logger.error(f"{IP_LIST_FILE} ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿ write_ip_list å·²æ­£ç¡®ç”Ÿæˆæ–‡ä»¶")
        return None

    start_time = time.time()
    try:
        with open(IP_LIST_FILE, "r", encoding="utf-8") as f:
            ip_lines = [line.strip() for line in f if line.strip()]
        total_nodes = len(ip_lines)
        logger.info(f"{IP_LIST_FILE} åŒ…å« {total_nodes} ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        logger.error(f"æ— æ³•è¯»å– {IP_LIST_FILE}: {e}")
        return None

    # è§£æ speedlimit å‚æ•°
    speed_limit = parse_speedlimit_from_script(SPEEDTEST_SCRIPT)
    
    logger.info("å¼€å§‹æµ‹é€Ÿ")
    system = platform.system().lower()
    is_termux_env = is_termux()
    try:
        if system == "windows":
            command = [SPEEDTEST_SCRIPT]
        elif is_termux_env:
            command = ["bash", SPEEDTEST_SCRIPT]  # Termux ä½¿ç”¨ bash æ‰§è¡Œ iptest.sh
        else:
            shell = shutil.which("bash") or shutil.which("sh") or "sh"
            command = ["stdbuf", "-oL", shell, SPEEDTEST_SCRIPT]
        
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            shell=False,
            encoding='utf-8',
            errors='replace'
        )
        stdout_lines, stderr_lines = [], []
        def read_stream(stream, lines, is_stderr=False):
            while True:
                line = stream.readline()
                if not line:
                    break
                lines.append(line)
                logger.info(line.strip())  # ç›´æ¥è®°å½•åŸå§‹è¾“å‡ºï¼Œæ— å‰ç¼€
                sys.stdout.flush()
        stdout_thread = threading.Thread(target=read_stream, args=(process.stdout, stdout_lines))
        stderr_thread = threading.Thread(target=read_stream, args=(process.stderr, stderr_lines, True))
        stdout_thread.start()
        stderr_thread.start()

        return_code = process.wait()
        stdout_thread.join()
        stderr_thread.join()
        stdout = ''.join(stdout_lines)
        stderr = ''.join(stderr_lines)
        if stdout:
            logger.info(f"iptest æ ‡å‡†è¾“å‡º: {stdout}")
        if stderr:
            logger.warning(f"iptest é”™è¯¯è¾“å‡º: {stderr}")

        logger.info(f"æµ‹é€Ÿå®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
        if return_code != 0:
            logger.error(f"æµ‹é€Ÿå¤±è´¥ï¼Œè¿”å›ç : {return_code}")
            return None
        if not os.path.exists(FINAL_CSV) or os.path.getsize(FINAL_CSV) < 10:
            logger.error(f"{FINAL_CSV} æœªç”Ÿæˆæˆ–å†…å®¹æ— æ•ˆ")
            return None
        
        # ç»Ÿè®¡ ip.csv çš„é€Ÿåº¦åˆ†å¸ƒ
        try:
            with open(FINAL_CSV, "r", encoding="utf-8") as f:
                reader = csv.reader(f)
                header = next(reader, None)
                speeds = []
                speed_col = 9  # ç¬¬ 10 åˆ—æ˜¯â€œä¸‹è½½é€Ÿåº¦MB/sâ€
                for row in reader:
                    if len(row) > speed_col and row[speed_col].strip():
                        try:
                            speeds.append(float(row[speed_col]))
                        except ValueError:
                            continue
                if speeds:
                    logger.info(f"ip.csv é€Ÿåº¦ç»Ÿè®¡: å¹³å‡={sum(speeds)/len(speeds):.2f} MB/s, "
                               f"æœ€å°={min(speeds):.2f} MB/s, æœ€å¤§={max(speeds):.2f} MB/s, "
                               f"èŠ‚ç‚¹æ•°={len(speeds)}")
        except Exception as e:
            logger.warning(f"æ— æ³•ç»Ÿè®¡ ip.csv é€Ÿåº¦åˆ†å¸ƒ: {e}")

        # åœ¨ Termux ç¯å¢ƒä¸­ï¼Œå¼ºåˆ¶è¿‡æ»¤ä½é€ŸèŠ‚ç‚¹
        if is_termux_env:
            logger.info(f"æ£€æµ‹åˆ° Termux ç¯å¢ƒï¼Œåº”ç”¨é€Ÿåº¦ä¸‹é™è¿‡æ»¤ (speedlimit={speed_limit} MB/s)")
            filter_ip_csv_by_speed(FINAL_CSV, speed_limit=speed_limit)  # ä½¿ç”¨åŠ¨æ€ speed_limit

        with open(FINAL_CSV, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f if line.strip()]
            node_count = len(lines) - 1 if lines else 0
            logger.info(f"{FINAL_CSV} åŒ…å« {node_count} ä¸ªèŠ‚ç‚¹")
        return FINAL_CSV
    except Exception as e:
        logger.error(f"æµ‹é€Ÿå¼‚å¸¸: {e}")
        return None

def filter_speed_and_deduplicate(csv_file: str, is_github_actions: bool):
    start_time = time.time()
    if not os.path.exists(csv_file):
        logger.info(f"{csv_file} ä¸å­˜åœ¨")
        return
    seen = set()
    final_rows = []
    try:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            header = next(reader, None)
            if not header:
                logger.error(f"{csv_file} æ²¡æœ‰æœ‰æ•ˆçš„è¡¨å¤´")
                return
            for row in reader:
                if len(row) < 2 or not row[0].strip():
                    continue
                key = (row[0], row[1])
                if key not in seen:
                    seen.add(key)
                    final_rows.append(row)
    except Exception as e:
        logger.error(f"æ— æ³•å¤„ç† {csv_file}: {e}")
        return
    if not final_rows:
        logger.info(f"æ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹")
        os.remove(csv_file)
        return
    try:
        final_rows.sort(key=lambda x: float(x[9]) if len(x) > 9 and x[9] and x[9].replace('.', '', 1).isdigit() else 0.0, reverse=True)
    except Exception as e:
        logger.warning(f"æ’åºå¤±è´¥: {e}")

    with open(csv_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(final_rows)
    logger.info(f"å·²ç”Ÿæˆ {csv_file}")

    logger.info(f"{csv_file} å¤„ç†å®Œæˆï¼Œ{len(final_rows)} ä¸ªæ•°æ®èŠ‚ç‚¹ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    return len(final_rows)

def generate_ips_file(csv_file: str, is_github_actions: bool):
    start_time = time.time()
    if not os.path.exists(csv_file):
        logger.info(f"{csv_file} ä¸å­˜åœ¨")
        return
    country_cache = load_country_cache()
    final_nodes = []
    try:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            next(reader)
            for row in reader:
                if len(row) < 2:
                    continue
                ip, port = row[0], row[1]
                if not is_valid_ip(ip) or not is_valid_port(port):
                    continue
                country = country_cache.get(ip, '')
                if not country:
                    country = get_country_from_ip(ip, country_cache)
                if DESIRED_COUNTRIES and country and country in DESIRED_COUNTRIES:
                    final_nodes.append((ip, int(port), country))
    except Exception as e:
        logger.error(f"æ— æ³•è¯»å– {csv_file}: {e}")
        return
    if not final_nodes:
        logger.info(f"æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹ï¼ˆDESIRED_COUNTRIES: {DESIRED_COUNTRIES}ï¼‰")
        return
    country_count = defaultdict(int)
    labeled_nodes = []
    for ip, port, country in sorted(final_nodes, key=lambda x: x[2] or 'ZZ'):
        country_count[country] += 1
        emoji, name = COUNTRY_LABELS.get(country, ('ğŸŒ', 'æœªçŸ¥'))
        label = f"{emoji} {name}-{country_count[country]}"
        labeled_nodes.append((ip, port, label))

    with open(IPS_FILE, "w", encoding="utf-8-sig") as f:
        for ip, port, label in labeled_nodes:
            f.write(f"{ip}:{port}#{label}\n")
    logger.info(f"å·²ç”Ÿæˆ {IPS_FILE}")

    logger.info(f"ç”Ÿæˆ {IPS_FILE}ï¼Œ{len(labeled_nodes)} ä¸ªæ•°æ®èŠ‚ç‚¹ (è€—æ—¶: {time.time() - start_time:.2f} ç§’)")
    logger.info(f"å›½å®¶åˆ†å¸ƒ: {dict(country_count)}")
    save_country_cache(country_cache)
    return len(labeled_nodes)

def validate_username(username: str) -> bool:
    """éªŒè¯ Git ç”¨æˆ·åæ ¼å¼"""
    if not username:
        logger.warning("ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
        return False
    if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9_-]*$', username):
        logger.warning("ç”¨æˆ·ååªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿æˆ–è¿å­—ç¬¦ï¼Œä¸”å¿…é¡»ä»¥å­—æ¯æˆ–æ•°å­—å¼€å¤´")
        return False
    return True

def validate_repo_name(repo_name: str) -> bool:
    """éªŒè¯ GitHub ä»“åº“åç§°æ ¼å¼"""
    if not repo_name:
        logger.warning("ä»“åº“åç§°ä¸èƒ½ä¸ºç©º")
        return False
    if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9_-]*$', repo_name):
        logger.warning("ä»“åº“åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿æˆ–è¿å­—ç¬¦ï¼Œä¸”å¿…é¡»ä»¥å­—æ¯æˆ–æ•°å­—å¼€å¤´")
        return False
    if '/' in repo_name:
        logger.warning("ä»“åº“åç§°ä¸èƒ½åŒ…å«æ–œæ ")
        return False
    return True

def validate_email(email: str) -> bool:
    """éªŒè¯é‚®ç®±æ ¼å¼"""
    if not email:
        logger.warning("é‚®ç®±ä¸èƒ½ä¸ºç©º")
        return False
    if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
        logger.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„é‚®ç®±åœ°å€")
        return False
    return True

def validate_remote_url(remote_url: str) -> bool:
    """éªŒè¯è¿œç¨‹ä»“åº“åœ°å€æ ¼å¼"""
    if not re.match(r'^git@github\.com:[a-zA-Z0-9][a-zA-Z0-9_-]*/[a-zA-Z0-9][a-zA-Z0-9_-]*\.git$', remote_url):
        logger.warning(f"è¿œç¨‹ä»“åº“åœ°å€æ ¼å¼æ— æ•ˆ: {remote_url}")
        return False
    return True

def verify_remote_url(remote_url: str) -> bool:
    """éªŒè¯è¿œç¨‹ä»“åº“æ˜¯å¦å¯è®¿é—®"""
    try:
        subprocess.run(
            ["git", "ls-remote", remote_url],
            check=True,
            capture_output=True,
            text=True
        )
        logger.info(f"è¿œç¨‹ä»“åº“ {remote_url} å¯è®¿é—®")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"æ— æ³•è®¿é—®è¿œç¨‹ä»“åº“ {remote_url}: {e.stderr}")
        return False

def verify_ssh_connection(ssh_key_path: str) -> bool:
    """éªŒè¯ä¸ GitHub çš„ SSH è¿æ¥æ˜¯å¦æœ‰æ•ˆ"""
    logger.info(f"å¼€å§‹éªŒè¯ SSH è¿æ¥åˆ° GitHubï¼Œä½¿ç”¨å¯†é’¥: {ssh_key_path}")
    if not os.path.exists(ssh_key_path):
        logger.error(f"SSH å¯†é’¥æ–‡ä»¶ {ssh_key_path} ä¸å­˜åœ¨")
        logger.info("è¯·ç”Ÿæˆ SSH å¯†é’¥ï¼š")
        logger.info("1. è¿è¡Œ 'ssh-keygen -t ed25519 -C \"your_email@example.com\"'")
        logger.info("2. å°†å…¬é’¥ (~/.ssh/id_ed25519.pub) æ·»åŠ åˆ° GitHub: https://github.com/settings/keys")
        return False

    if platform.system().lower() != "windows":
        try:
            file_stat = os.stat(ssh_key_path)
            if file_stat.st_mode & (stat.S_IRWXG | stat.S_IRWXO):
                logger.warning(f"SSH å¯†é’¥æ–‡ä»¶ {ssh_key_path} æƒé™è¿‡äºå®½æ¾ï¼Œå»ºè®®è®¾ç½®ä¸º 600")
                logger.info("ä¿®å¤æƒé™ï¼šè¿è¡Œ 'chmod 600 {ssh_key_path}'")
        except OSError as e:
            logger.warning(f"æ— æ³•æ£€æŸ¥ SSH å¯†é’¥æ–‡ä»¶æƒé™: {e}")

    try:
        result = subprocess.run(
            ["ssh", "-T", "-o", "StrictHostKeyChecking=no", "-i", ssh_key_path, "git@github.com"],
            capture_output=True,
            text=True,
            check=False
        )
        output = (result.stdout + result.stderr).lower()
        if "successfully authenticated" in output:
            logger.info("SSH è¿æ¥åˆ° GitHub éªŒè¯æˆåŠŸ")
            return True
        else:
            logger.warning(f"SSH è¿æ¥éªŒè¯å¤±è´¥ï¼Œè¾“å‡º: {output.strip()}")
            logger.info("è¯·ç¡®ä¿ä»¥ä¸‹æ­¥éª¤å·²å®Œæˆï¼š")
            logger.info("1. SSH ç§é’¥ ({ssh_key_path}) å­˜åœ¨ä¸”æœ‰æ•ˆ")
            logger.info("2. å¯¹åº”çš„å…¬é’¥å·²æ·»åŠ åˆ° GitHub: https://github.com/settings/keys")
            logger.info("3. æ£€æŸ¥ SSH ä»£ç†ï¼ˆå¦‚æœä½¿ç”¨ï¼‰ï¼šè¿è¡Œ 'ssh-add {ssh_key_path}'")
            return False
    except subprocess.CalledProcessError as e:
        logger.error(f"æ— æ³•éªŒè¯ SSH è¿æ¥: {e.stderr}")
        logger.info("å¯èƒ½çš„åŸå› ï¼š")
        logger.info("- SSH å®¢æˆ·ç«¯æœªå®‰è£…æˆ–é…ç½®é”™è¯¯")
        logger.info("- ç½‘ç»œè¿æ¥é—®é¢˜")
        logger.info("- SSH å¯†é’¥æœªæ­£ç¡®æ·»åŠ åˆ° ssh-agentï¼ˆå°è¯• 'ssh-add {ssh_key_path}'ï¼‰")
        return False
    except FileNotFoundError:
        logger.error("SSH å®¢æˆ·ç«¯æœªå®‰è£…ï¼Œè¯·å®‰è£… OpenSSH")
        logger.info("Ubuntu: sudo apt-get install openssh-client")
        logger.info("Windows: ç¡®ä¿ Git Bash æˆ– OpenSSH å·²å®‰è£…")
        return False
    except Exception as e:
        logger.error(f"éªŒè¯ SSH è¿æ¥æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return False

def load_config() -> Dict[str, str]:
    """åŠ è½½å¹¶éªŒè¯ .gitconfig.json æ–‡ä»¶"""
    if not os.path.exists(CONFIG_FILE):
        logger.info(f"æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CONFIG_FILE}ï¼Œå°†é‡æ–°æç¤ºè¾“å…¥")
        return {}
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            required_fields = ['user_name', 'user_email', 'repo_name', 'ssh_key_path', 'git_user_name']
            missing_fields = [field for field in required_fields if field not in config]
            if missing_fields:
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ç¼ºå°‘å­—æ®µ: {missing_fields}")
                return {}

            if not validate_username(config['user_name']):
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸­ user_name æ— æ•ˆ: {config['user_name']}")
                return {}
            if not validate_email(config['user_email']):
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸­ user_email æ— æ•ˆ: {config['user_email']}")
                return {}
            if not validate_username(config['git_user_name']):
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸­ git_user_name æ— æ•ˆ: {config['git_user_name']}")
                return {}
            if not validate_repo_name(config['repo_name']):
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸­ repo_name æ— æ•ˆ: {config['repo_name']}")
                return {}
            if not os.path.exists(config['ssh_key_path']):
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸­ ssh_key_path ä¸å­˜åœ¨: {config['ssh_key_path']}")
                return {}
            if not os.access(config['ssh_key_path'], os.R_OK):
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸­ ssh_key_path ä¸å¯è¯»: {config['ssh_key_path']}")
                return {}

            remote_url = f"git@github.com:{config['git_user_name']}/{config['repo_name']}.git"
            if not validate_remote_url(remote_url):
                logger.warning(f"æ„é€ çš„è¿œç¨‹åœ°å€æ— æ•ˆ: {remote_url}")
                return {}
            if not verify_remote_url(remote_url):
                logger.warning(f"è¿œç¨‹ä»“åº“ä¸å¯è®¿é—®: {remote_url}")
                return {}
            if not verify_ssh_connection(config['ssh_key_path']):
                logger.warning("SSH è¿æ¥éªŒè¯å¤±è´¥")
                return {}

            logger.info("å·²ä»ç¼“å­˜åŠ è½½ Git é…ç½®")
            return config
    except json.JSONDecodeError as e:
        logger.error(f"è§£æ {CONFIG_FILE} å¤±è´¥ï¼ŒJSON æ ¼å¼é”™è¯¯: {e}")
        return {}
    except PermissionError as e:
        logger.error(f"æ— æ³•è¯»å– {CONFIG_FILE}ï¼Œæƒé™é”™è¯¯: {e}")
        return {}
    except Exception as e:
        logger.error(f"åŠ è½½ {CONFIG_FILE} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return {}

def save_config(config: Dict[str, str]):
    """ä¿å­˜ Git é…ç½®åˆ° .gitconfig.json"""
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        os.chmod(CONFIG_FILE, stat.S_IRUSR | stat.S_IWUSR)
        logger.info(f"Git é…ç½®å·²ä¿å­˜åˆ° {CONFIG_FILE}")
    except Exception as e:
        logger.error(f"æ— æ³•ä¿å­˜ç¼“å­˜æ–‡ä»¶ {CONFIG_FILE}: {e}")
        sys.exit(1)

def prompt_git_config() -> Dict[str, str]:
    """æç¤ºç”¨æˆ·è¾“å…¥ Git é…ç½®"""
    logger.info("éœ€è¦é…ç½® Git ä¿¡æ¯")
    user_name = input("è¯·è¾“å…¥ Git ç”¨æˆ·å: ").strip()
    while not validate_username(user_name):
        user_name = input("è¯·è¾“å…¥ Git ç”¨æˆ·å: ").strip()

    user_email = input("è¯·è¾“å…¥ Git é‚®ç®±: ").strip()
    while not validate_email(user_email):
        user_email = input("è¯·è¾“å…¥ Git é‚®ç®±: ").strip()

    git_user_name = input("è¯·è¾“å…¥ GitHub ç”¨æˆ·å: ").strip()
    while not validate_username(git_user_name):
        git_user_name = input("è¯·è¾“å…¥ GitHub ç”¨æˆ·å: ").strip()

    repo_name = input("è¯·è¾“å…¥ GitHub ä»“åº“åç§°: ").strip()
    while not validate_repo_name(repo_name):
        repo_name = input("è¯·è¾“å…¥ GitHub ä»“åº“åç§°: ").strip()

    remote_url = f"git@github.com:{git_user_name}/{repo_name}.git"
    if not validate_remote_url(remote_url):
        logger.error(f"æ„é€ çš„è¿œç¨‹ä»“åº“åœ°å€æ— æ•ˆ: {remote_url}")
        sys.exit(1)
    if not verify_remote_url(remote_url):
        logger.error(f"è¿œç¨‹ä»“åº“ä¸å¯è®¿é—®: {remote_url}")
        logger.info("è¯·ç¡®ä¿ï¼š1. ä»“åº“å­˜åœ¨ï¼›2. GitHub ç”¨æˆ·åæ­£ç¡®ï¼›3. ä½ æœ‰è®¿é—®æƒé™")
        sys.exit(1)

    ssh_key_path = SSH_KEY_PATH
    if not os.path.exists(ssh_key_path) or not verify_ssh_connection(ssh_key_path):
        logger.info("SSH å¯†é’¥æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œè¯·ç”Ÿæˆæ–°å¯†é’¥")
        ssh_key_path = generate_ssh_key()

    return {
        "user_name": user_name,
        "user_email": user_email,
        "repo_name": repo_name,
        "ssh_key_path": ssh_key_path,
        "git_user_name": git_user_name
    }

def generate_ssh_key() -> str:
    """ç”Ÿæˆ SSH å¯†é’¥å¹¶éªŒè¯è¿æ¥"""
    ssh_dir = os.path.expanduser("~/.ssh")
    private_key_path = SSH_KEY_PATH
    public_key_path = f"{private_key_path}.pub"

    if os.path.exists(private_key_path) and os.path.exists(public_key_path):
        logger.info(f"SSH å¯†é’¥å·²å­˜åœ¨: {private_key_path}")
        if verify_ssh_connection(private_key_path):
            return private_key_path
        logger.info("ç°æœ‰ SSH å¯†é’¥æ— æ³•è¿æ¥åˆ° GitHubï¼Œå°†ç”Ÿæˆæ–°å¯†é’¥")

    try:
        os.makedirs(ssh_dir, mode=0o700, exist_ok=True)
        logger.info(f"ç”Ÿæˆæ–°çš„ SSH å¯†é’¥: {private_key_path}")
        email = input("è¯·è¾“å…¥ç”¨äº SSH å¯†é’¥çš„é‚®ç®±ï¼ˆç”¨äºæ³¨é‡Šï¼‰: ").strip()
        while not validate_email(email):
            email = input("è¯·è¾“å…¥æœ‰æ•ˆçš„é‚®ç®±: ").strip()

        subprocess.run(
            ["ssh-keygen", "-t", "ed25519", "-C", email, "-f", private_key_path, "-N", ""],
            check=True,
            capture_output=True,
            text=True
        )
        logger.info(f"SSH å¯†é’¥ç”ŸæˆæˆåŠŸ: {private_key_path}")

        if platform.system().lower() != "windows":
            os.chmod(private_key_path, 0o600)
            os.chmod(public_key_path, 0o644)
            logger.info(f"å·²è®¾ç½®å¯†é’¥æ–‡ä»¶æƒé™: {private_key_path} (600), {public_key_path} (644)")

        with open(public_key_path, "r", encoding="utf-8") as f:
            public_key = f.read().strip()
        logger.info("SSH å…¬é’¥å†…å®¹å¦‚ä¸‹ï¼Œè¯·æ·»åŠ åˆ° GitHub: https://github.com/settings/keys")
        logger.info(public_key)
        input("è¯·å°†ä»¥ä¸Šå…¬é’¥æ·»åŠ åˆ° GitHub åæŒ‰ Enter ç»§ç»­...")

        if not verify_ssh_connection(private_key_path):
            logger.error("æ–°ç”Ÿæˆçš„ SSH å¯†é’¥ä»æ— æ³•è¿æ¥åˆ° GitHub")
            sys.exit(1)

        logger.info("SSH å¯†é’¥éªŒè¯æˆåŠŸ")
        return private_key_path
    except subprocess.CalledProcessError as e:
        logger.error(f"ç”Ÿæˆ SSH å¯†é’¥å¤±è´¥: {e.stderr}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç”Ÿæˆ SSH å¯†é’¥æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        sys.exit(1)

def setup_git_config(is_github_actions: bool = False):
    """è®¾ç½® Git é…ç½®"""
    if is_github_actions:
        logger.info("æ£€æµ‹åˆ° GitHub Actions ç¯å¢ƒï¼Œè·³è¿‡äº¤äº’å¼ Git é…ç½®")
        try:
            subprocess.run(["git", "config", "--global", "user.name", "github-actions[bot]"], check=True)
            subprocess.run(["git", "config", "--global", "user.email", "github-actions[bot]@users.noreply.github.com"], check=True)
            logger.info("å·²è®¾ç½® GitHub Actions é»˜è®¤ Git é…ç½®")
            return
        except subprocess.CalledProcessError as e:
            logger.error(f"è®¾ç½® GitHub Actions Git é…ç½®å¤±è´¥: {e}")
            sys.exit(1)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å…¨å±€ Git é…ç½®
    try:
        current_user = subprocess.run(["git", "config", "--global", "user.name"], capture_output=True, text=True, check=False).stdout.strip()
        current_email = subprocess.run(["git", "config", "--global", "user.email"], capture_output=True, text=True, check=False).stdout.strip()
        if current_user and current_email:
            logger.info(f"æ£€æµ‹åˆ°ç°æœ‰ Git å…¨å±€é…ç½®: user.name={current_user}, user.email={current_email}")
            config = load_config()
            if config:
                logger.info("ä½¿ç”¨ç¼“å­˜çš„ Git é…ç½®")
                return
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç¼“å­˜é…ç½®ï¼Œå°†æç¤ºè¾“å…¥")
        else:
            logger.info("æœªæ£€æµ‹åˆ° Git å…¨å±€é…ç½®ï¼Œå°†æç¤ºè¾“å…¥")
    except subprocess.CalledProcessError as e:
        logger.warning(f"æ£€æŸ¥ Git å…¨å±€é…ç½®å¤±è´¥: {e}")

    # åŠ è½½æˆ–æç¤ºé…ç½®
    config = load_config()
    if not config:
        config = prompt_git_config()
        save_config(config)

    # è®¾ç½® Git å…¨å±€é…ç½®
    try:
        subprocess.run(["git", "config", "--global", "user.name", config['user_name']], check=True)
        subprocess.run(["git", "config", "--global", "user.email", config['user_email']], check=True)
        logger.info(f"å·²è®¾ç½® Git å…¨å±€é…ç½®: user.name={config['user_name']}, user.email={config['user_email']}")
    except subprocess.CalledProcessError as e:
        logger.error(f"è®¾ç½® Git å…¨å±€é…ç½®å¤±è´¥: {e}")
        sys.exit(1)

def commit_and_push(is_github_actions: bool = False, no_push: bool = False):
    """æäº¤å¹¶æ¨é€æ›´æ”¹åˆ° GitHub"""
    if no_push:
        logger.info("æ£€æµ‹åˆ° --no-push å‚æ•°ï¼Œè·³è¿‡ Git æäº¤å’Œæ¨é€")
        return
    config = load_config()
    if not config:
        logger.error(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„ Git é…ç½®ï¼Œè¯·ç¡®ä¿ {CONFIG_FILE} å­˜åœ¨ä¸”æœ‰æ•ˆ")
        sys.exit(1)

    remote_url = f"git@github.com:{config['git_user_name']}/{config['repo_name']}.git"
    if not validate_remote_url(remote_url):
        logger.error(f"è¿œç¨‹ä»“åº“åœ°å€æ— æ•ˆ: {remote_url}")
        sys.exit(1)
    if not verify_remote_url(remote_url):
        logger.error(f"è¿œç¨‹ä»“åº“ {remote_url} ä¸å¯è®¿é—®")
        sys.exit(1)
    if not verify_ssh_connection(config['ssh_key_path']):
        logger.error("SSH è¿æ¥éªŒè¯å¤±è´¥")
        sys.exit(1)

    try:
        # åˆå§‹åŒ– Git ä»“åº“
        if not os.path.exists(".git"):
            subprocess.run(["git", "init"], check=True)
            logger.info("å·²åˆå§‹åŒ– Git ä»“åº“")
        else:
            logger.info("Git ä»“åº“å·²å­˜åœ¨")

        # æ£€æŸ¥å·¥ä½œåŒºçŠ¶æ€
        status_result = subprocess.run(
            ["git", "status", "--porcelain", "--untracked-files=no"],
            capture_output=True,
            text=True,
            check=True
        )
        if "UU" in status_result.stdout:
            logger.warning("æ£€æµ‹åˆ°æœªè§£å†³çš„åˆå¹¶å†²çªï¼Œè¯·æ‰‹åŠ¨è§£å†³ï¼š")
            logger.warning("1. è¿è¡Œ 'git status' æŸ¥çœ‹å†²çªæ–‡ä»¶")
            logger.warning("2. è§£å†³å†²çªåè¿è¡Œ 'git add <file>'")
            logger.warning("3. æäº¤ 'git commit'")
            return

        # è®¾ç½®è¿œç¨‹ä»“åº“
        try:
            subprocess.run(["git", "remote", "set-url", "origin", remote_url], check=True)
        except subprocess.CalledProcessError:
            subprocess.run(["git", "remote", "add", "origin", remote_url], check=True)
            logger.info(f"å·²è®¾ç½®è¿œç¨‹ä»“åº“: {remote_url}")

        # æ·»åŠ æ–‡ä»¶
        files_to_commit = [IPS_FILE, FINAL_CSV]
        for file in files_to_commit:
            if os.path.exists(file):
                subprocess.run(["git", "add", file], check=True)
                logger.info(f"å·²æ·»åŠ æ–‡ä»¶åˆ° Git: {file}")
            else:
                logger.warning(f"æ–‡ä»¶ {file} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ·»åŠ ")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ”¹
        status_result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True,
            text=True,
            check=True
        )
        if not status_result.stdout.strip():
            logger.info("æ²¡æœ‰æ›´æ”¹éœ€è¦æäº¤")
            return

        # æäº¤æ›´æ”¹
        commit_message = "Update IP lists and test results" if is_github_actions else "Update IP lists and test results via script"
        subprocess.run(["git", "commit", "-m", commit_message], check=True)
        logger.info(f"å·²æäº¤æ›´æ”¹: {commit_message}")

        # æ¨é€
        branch = "main" if is_github_actions else "main"
        subprocess.run(["git", "push", "origin", branch], check=True)
        logger.info(f"å·²æ¨é€æ›´æ”¹åˆ°è¿œç¨‹ä»“åº“: {remote_url} (åˆ†æ”¯: {branch})")
    except subprocess.CalledProcessError as e:
        logger.error(f"Git æ“ä½œå¤±è´¥: {e.stderr or str(e)}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æäº¤å’Œæ¨é€è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="IP æµ‹è¯•å’Œç­›é€‰è„šæœ¬")
    parser.add_argument("--input-file", type=str, default=INPUT_FILE, help=f"è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {INPUT_FILE})")
    parser.add_argument(
        "--url",
        type=str,
        action="append",
        default=INPUT_URLS,
        help=f"è¾“å…¥ URL åˆ—è¡¨ (é»˜è®¤: {INPUT_URLS})"
    )
    parser.add_argument("--offline", action="store_true", help="ç¦»çº¿æ¨¡å¼ï¼Œä¸ä¸‹è½½ GeoIP æ•°æ®åº“")
    parser.add_argument("--update-geoip", action="store_true", help="å¼ºåˆ¶æ›´æ–° GeoIP æ•°æ®åº“")
    parser.add_argument("--no-push", action="store_true", help="ç¦ç”¨ Git æäº¤å’Œæ¨é€")
    args = parser.parse_args()

    is_github_actions = os.getenv("GITHUB_ACTIONS") == "true"
    logger.info(f"è¿è¡Œç¯å¢ƒ: {'GitHub Actions' if is_github_actions else 'æœ¬åœ°'}, ç¦»çº¿æ¨¡å¼: {args.offline}, æ›´æ–° GeoIP: {args.update_geoip}")

    # è®¾ç½®è™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
    setup_and_activate_venv()

    # æ£€æŸ¥ä¾èµ–
    check_dependencies(offline=args.offline, update_geoip=args.update_geoip)

    # è®¾ç½® Git é…ç½®
    setup_git_config(is_github_actions=is_github_actions)

    # å¤„ç†è¾“å…¥
    ip_ports = []

    # 1. ä»æœ¬åœ° input.csv è·å–èŠ‚ç‚¹
    if os.path.exists(args.input_file):
        ip_ports = extract_ip_ports_from_file(args.input_file)
        if ip_ports:
            logger.info(f"ä»æœ¬åœ°æ–‡ä»¶ {args.input_file} æå–åˆ° {len(ip_ports)} ä¸ªèŠ‚ç‚¹")
        else:
            logger.warning(f"æœ¬åœ°æ–‡ä»¶ {args.input_file} æ— æœ‰æ•ˆèŠ‚ç‚¹")
    else:
        logger.info(f"æœ¬åœ°æ–‡ä»¶ {args.input_file} ä¸å­˜åœ¨ï¼Œå°è¯•ä» URL å’Œç½‘é¡µè·å–")
        ip_ports = fetch_all_sources(args)
        if ip_ports:
            logger.info(f"ä»æ‰€æœ‰åœ¨çº¿æ¥æºå…±æå–åˆ° {len(ip_ports)} ä¸ªèŠ‚ç‚¹")
        else:
            logger.warning(f"æ— æ³•ä» INPUT_URLS {args.url} æˆ– WEB_URLS {WEB_URLS} è·å–æœ‰æ•ˆèŠ‚ç‚¹")

    # å»é‡
    ip_ports = list(dict.fromkeys(ip_ports))
    logger.info(f"å»é‡åæ€»è®¡ {len(ip_ports)} ä¸ªèŠ‚ç‚¹")

    if not ip_ports:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ IP å’Œç«¯å£æ•°æ®")
        sys.exit(1)

    # å†™å…¥ IP åˆ—è¡¨
    ip_list_file = write_ip_list(ip_ports, is_github_actions=is_github_actions)
    if not ip_list_file:
        logger.error("æ— æ³•ç”Ÿæˆ IP åˆ—è¡¨")
        sys.exit(1)

    # è¿è¡Œæµ‹é€Ÿ
    csv_file = run_speed_test()
    if not csv_file:
        logger.error("æµ‹é€Ÿå¤±è´¥")
        sys.exit(1)

    # è¿‡æ»¤å’Œå»é‡
    node_count = filter_speed_and_deduplicate(csv_file, is_github_actions=is_github_actions)
    if not node_count:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„èŠ‚ç‚¹")
        sys.exit(1)

    # ç”Ÿæˆæœ€ç»ˆ IPs æ–‡ä»¶
    final_node_count = generate_ips_file(csv_file, is_github_actions=is_github_actions)
    if not final_node_count:
        logger.error("æ— æ³•ç”Ÿæˆæœ€ç»ˆ IPs æ–‡ä»¶")
        sys.exit(1)

    # æäº¤å¹¶æ¨é€
    commit_and_push(is_github_actions=is_github_actions, no_push=args.no_push)

    logger.info("è„šæœ¬æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­è„šæœ¬æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
